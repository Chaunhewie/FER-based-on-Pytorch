{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 72379,
     "status": "ok",
     "timestamp": 1556208136676,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "8tTKUKdNF90j",
    "outputId": "03ce0c5f-4138-4f9d-8823-2c696b5b5c50"
   },
   "outputs": [],
   "source": [
    "# 获取授权\n",
    "!apt-get install -y -qq software-properties-common module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9247,
     "status": "ok",
     "timestamp": 1556208150144,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "eW8YCA7BGVMG",
    "outputId": "2526cb43-c930-4ced-d6b4-92fafab78ef8"
   },
   "outputs": [],
   "source": [
    "# 指定Google Drive云端硬盘的根目录，名为drive\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhgBEpBhIcap"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"drive/my_scripts\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10969,
     "status": "ok",
     "timestamp": 1556208155069,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "cEpeEkcdLDt-",
    "outputId": "e3beea04-3f28-452a-bfd4-ed0dee7c34dd"
   },
   "outputs": [],
   "source": [
    "# 云端默认没有face_recognition\n",
    "!pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20650,
     "status": "ok",
     "timestamp": 1556208168510,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "pJp72yYwKBAY",
    "outputId": "69753eb4-ad68-44fa-d5e5-7fbe734c05e0"
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "!nvidia-smi\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1556210324383,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "e_WoO8cMFPbl",
    "outputId": "5f5ec469-2862-42e2-89dc-8d27688b146b"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "# 用于重新加载模块\n",
    "import importlib\n",
    "\n",
    "import networks.ACNN as net_ACNN\n",
    "importlib.reload(net_ACNN)\n",
    "ACNN = net_ACNN.ACNN\n",
    "import networks.ACCNN as net_ACCNN\n",
    "importlib.reload(net_ACCNN)\n",
    "ACCNN = net_ACCNN.ACCNN\n",
    "from networks.AlexNet import AlexNet\n",
    "from networks.VGG import vgg11_bn, vgg13_bn, vgg16_bn, vgg19_bn\n",
    "from networks.ResNet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "import dal.JAFFE_DataSet as JAFFE_DataSet\n",
    "import dal.CKPlus_DataSet as CKPlus_DataSet\n",
    "import dal.FER2013_DataSet as FER2013_DataSet\n",
    "import dal.RAF_DataSet as RAF_DataSet\n",
    "import dal.Data_Prefetcher as Data_Prefetcher\n",
    "import dal.Raw_DataSet as Raw_DataSet\n",
    "# importlib.reload(JAFFE_DataSet)\n",
    "# importlib.reload(CKPlus_DataSet)\n",
    "# importlib.reload(FER2013_DataSet)\n",
    "# importlib.reload(RAF_DataSet)\n",
    "# importlib.reload(Data_Prefetcher)\n",
    "# importlib.reload(Raw_DataSet)\n",
    "JAFFE = JAFFE_DataSet.JAFFE\n",
    "CKPlus = CKPlus_DataSet.CKPlus\n",
    "FER2013 = FER2013_DataSet.FER2013\n",
    "RAF = RAF_DataSet.RAF\n",
    "Prefetcher = Data_Prefetcher.DataPrefetcher\n",
    "RawDataSet = Raw_DataSet.RawDataSet\n",
    "import transforms.transforms as transforms\n",
    "import utils.utils as utils\n",
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")  # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多\n",
    "print('cuda available: ', use_cuda)\n",
    "print('using DEVICE: ', DEVICE)\n",
    "enabled_nets = [\"ACNN\", \"ACCNN\", \"AlexNet\", \"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\", \"ResNet18\", \"ResNet34\", \"ResNet50\",\n",
    "                \"ResNet101\", \"ResNet152\"]\n",
    "enabled_datasets = [\"JAFFE\", \"CK+\", \"FER2013\", \"RAF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0scksgMFPbw"
   },
   "outputs": [],
   "source": [
    "class OPT:\n",
    "    def __init__(self):\n",
    "        # Other Parameters\n",
    "        # 是否使用面部标记点进行训练\n",
    "        self.fl = True\n",
    "        # 存储的模型序号\n",
    "        self.save_number = 5\n",
    "        # 批次大小\n",
    "        self.bs = 32\n",
    "        # 学习率\n",
    "        self.lr = 0.01\n",
    "        # epoch\n",
    "        self.epoch = 200\n",
    "        # 每次获得到更优的准确率后，会进行一次存储，此选项选择是否从上次存储位置继续\n",
    "        self.resume = True\n",
    "        # 表示一开始的lrd_je个epoch，lr增大来解决一开始的收敛缓慢问题\n",
    "        self.lre_je = 20\n",
    "        # 表示默认从第 $lrd_se 次epoch开始进行lr的递减\n",
    "        self.lrd_se = 180\n",
    "        # 表示默认每经过2次epoch进行一次递减\n",
    "        self.lrd_s = 2\n",
    "        # 表示每次的lr的递减率，默认每递减一次乘一次0.9\n",
    "        self.lrd_r = 0.9\n",
    "        self.pred_err_epoch_max = 1\n",
    "        self.pred_err_lr_decay = 1\n",
    "        \n",
    "#         self.model, self.epoch, self.bs = 'ACNN', 1000, 128\n",
    "        self.model, self.epoch, self.bs = 'ACCNN', 1600, 100\n",
    "#         self.model, self.epoch, self.bs = 'AlexNet', 500, 32\n",
    "#         self.model, self.epoch, self.bs = 'VGG11', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'VGG13', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'VGG16', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'VGG19', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'ResNet18', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'ResNet34', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'ResNet50', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'ResNet101', 200, 32\n",
    "#         self.model, self.epoch, self.bs = 'ResNet152', 200, 32\n",
    "        self.lre_je = int(self.epoch*0.1)\n",
    "        self.lrd_se = 1400\n",
    "        self.lrd_s = 20\n",
    "        self.pred_err_epoch_max = 5\n",
    "        \n",
    "#         self.dataset, self.tr_using_crop = 'FER2013', False\n",
    "#         self.dataset, self.tr_using_crop = 'JAFFE', False\n",
    "#         self.dataset, self.tr_using_crop = 'CK+', False\n",
    "        self.dataset, self.tr_using_crop = 'RAF', False\n",
    "opt = OPT()\n",
    "\n",
    "\n",
    "train_acc_map = {'best_acc': 0, 'best_acc_epoch': -1, 0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "test_acc_map = {'best_acc': 0, 'best_acc_epoch': -1, 0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "Train_acc, Test_acc = 0., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56468,
     "status": "ok",
     "timestamp": 1556210388903,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "kJSsPjSmPbz9",
    "outputId": "279e7da1-72d9-4f12-b994-acc1b48683b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reset_dataset = True\n",
    "if reset_dataset:\n",
    "    criterion, target_type = nn.CrossEntropyLoss(), 'ls'\n",
    "    print(\"------------Preparing Data...----------------\")\n",
    "    if opt.dataset == \"JAFFE\":\n",
    "        train_data = JAFFE(is_train=True, transform=None, target_type=target_type, using_fl=opt.fl)\n",
    "        test_data = JAFFE(is_train=False, transform=None, target_type=target_type, using_fl=opt.fl)\n",
    "    elif opt.dataset == \"CK+\":\n",
    "        train_data = CKPlus(is_train=True, transform=None, target_type=target_type, using_fl=opt.fl)\n",
    "        test_data = CKPlus(is_train=False, transform=None, target_type=target_type, using_fl=opt.fl)\n",
    "    elif opt.dataset == \"FER2013\":\n",
    "        train_data = FER2013(is_train=True, private_test=True, transform=None, target_type=target_type,\n",
    "                             using_fl=opt.fl)\n",
    "        test_data = FER2013(is_train=False, private_test=True, transform=None, target_type=target_type,\n",
    "                            using_fl=opt.fl)\n",
    "    elif opt.dataset == \"RAF\":\n",
    "        train_data = RAF(is_train=True, transform=None, target_type=target_type,\n",
    "                             using_fl=opt.fl)\n",
    "        test_data = RAF(is_train=False, transform=None, target_type=target_type,\n",
    "                            using_fl=opt.fl)\n",
    "    else:\n",
    "        assert(\"opt.dataset should be in %s, but got %s\" % (enabled_datasets, opt.dataset))\n",
    "    print(\"------------%s Data Already be Prepared------------\" % opt.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 7\n",
    "net_to_save_dir = \"Saved_Models\"\n",
    "net_to_save_path = os.path.join(net_to_save_dir, str(opt.save_number), opt.dataset+'_'+opt.model+'_'+str(opt.save_number))\n",
    "if opt.fl:\n",
    "    saved_model_name = \"Best_model_fl.t7\"\n",
    "    saved_temp_model_name = \"Best_model_fl_temp.t7\"\n",
    "    model_over_flag_name = \"__%d_success_fl__\" % (opt.epoch)\n",
    "    history_file_name = \"history_fl.txt\"\n",
    "else:\n",
    "    saved_model_name = \"Best_model.t7\"\n",
    "    saved_temp_model_name = \"Best_model_temp.t7\"\n",
    "    model_over_flag_name = \"__%d_success__\" % (opt.epoch)\n",
    "    history_file_name = \"history.txt\"\n",
    "over_flag = False  # 如果已经成功训练完，就可以结束了\n",
    "TEMP_EPOCH = 2  # 用于暂时存储，每TEMP_EPOCH次存一次\n",
    "temp_internal = TEMP_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54545,
     "status": "ok",
     "timestamp": 1556210388905,
     "user": {
      "displayName": "Chaunhewie Tian",
      "photoUrl": "https://lh4.googleusercontent.com/-K_cnsi_ZdWw/AAAAAAAAAAI/AAAAAAAAAA8/VXOJ9SNCpuw/s64/photo.jpg",
      "userId": "07759722813896836245"
     },
     "user_tz": -480
    },
    "id": "fRl8SEjlFPby",
    "outputId": "1b99ad62-751b-4c91-b4aa-ca348f56c5d8"
   },
   "outputs": [],
   "source": [
    "print(\"------------Preparing Model...----------------\")\n",
    "if opt.model.lower() == \"ACNN\".lower():\n",
    "    net = ACNN(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"ACCNN\".lower():\n",
    "    net = ACCNN(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"AlexNet\".lower():\n",
    "    net = AlexNet(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"VGG11\".lower():\n",
    "    net = vgg11_bn(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"VGG13\".lower():\n",
    "    net = vgg13_bn(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"VGG16\".lower():\n",
    "    net = vgg16_bn(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"VGG19\".lower():\n",
    "    net = vgg19_bn(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"ResNet18\".lower():\n",
    "    net = resnet18(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"ResNet34\".lower():\n",
    "    net = resnet34(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"ResNet50\".lower():\n",
    "    net = resnet50(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"ResNet101\".lower():\n",
    "    net = resnet101(n_classes=n_classes).to(DEVICE)\n",
    "elif opt.model.lower() == \"ResNet152\".lower():\n",
    "    net = resnet152(n_classes=n_classes).to(DEVICE)\n",
    "else:\n",
    "    net = None\n",
    "    assert(\"opt.model should be in %s, but got %s\" % (enabled_nets, opt.model))\n",
    "start_epoch = 0\n",
    "if opt.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Loading Model Parameters...')\n",
    "#     if os.path.exists(os.path.join(net_to_save_path, saved_model_name)):\n",
    "    if os.path.exists(os.path.join(net_to_save_path, saved_temp_model_name)):\n",
    "        if os.path.exists(os.path.join(net_to_save_path, model_over_flag_name)):\n",
    "            print(\"Model trained over flag checked!\")\n",
    "            over_flag = True\n",
    "        assert os.path.isdir(net_to_save_path), 'Error: no checkpoint directory found!'\n",
    "        checkpoint = torch.load(os.path.join(net_to_save_path, saved_temp_model_name))\n",
    "#         checkpoint = torch.load(os.path.join(net_to_save_path, saved_model_name))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        test_acc_map['best_acc'] = checkpoint['best_test_acc']\n",
    "        test_acc_map['best_acc_epoch'] = checkpoint['best_test_acc_epoch']\n",
    "        if 'best_train_acc' in checkpoint:\n",
    "            train_acc_map['best_acc'] = checkpoint['best_train_acc']\n",
    "            train_acc_map['best_acc_epoch'] = checkpoint['best_train_acc_epoch']\n",
    "        start_epoch = checkpoint['cur_epoch'] + 1\n",
    "    else:\n",
    "        print(\"Checkout File not Found, No initialization.\")\n",
    "print(\"------------%s Model Already be Prepared------------\" % opt.model)\n",
    "\n",
    "# for gray images\n",
    "IMG_MEAN = [0.449]\n",
    "IMG_STD = [0.226]\n",
    "\n",
    "crop_img_size = int(net.input_size*1.2)\n",
    "input_img_size = net.input_size\n",
    "transform_using_crop = opt.tr_using_crop\n",
    "if transform_using_crop:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(crop_img_size),\n",
    "        transforms.TenCrop(input_img_size),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(IMG_MEAN, IMG_STD)(\n",
    "                                                     transforms.ToTensor()(\n",
    "                                                     transforms.RandomHorizontalFlip()(\n",
    "                                                     transforms.RandomRotation(30)(crop)))) for crop in crops])),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(crop_img_size),\n",
    "        transforms.TenCrop(input_img_size),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(IMG_MEAN, IMG_STD)(\n",
    "                                                     transforms.ToTensor()(crop)) for crop in crops])),\n",
    "    ])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(input_img_size),  # 缩放将图片的最小边缩放为 input_img_size，因此如果输入是非正方形的，那么输出也不是正方形的\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMG_MEAN, IMG_STD),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(input_img_size),  # 缩放将图片的最小边缩放为 input_img_size，因此如果输入是非正方形的，那么输出也不是正方形的\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMG_MEAN, IMG_STD),\n",
    "    ])\n",
    "# 随机梯度下降\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=opt.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "train_data.set_transform(transform_train)\n",
    "test_data.set_transform(transform_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=opt.bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbnQAQuBP0BD"
   },
   "outputs": [],
   "source": [
    "# 测试数据集输出的正确性\n",
    "\n",
    "# itr = enumerate(train_loader)\n",
    "# bs, (inputs, targets) = next(itr)\n",
    "# inputs, targets = inputs.numpy(), targets.numpy()\n",
    "\n",
    "# 升级 Prefetcher 后\n",
    "train_prefetcher = Prefetcher(train_loader)\n",
    "inputs, targets = train_prefetcher.next()\n",
    "inputs, targets = inputs.cpu(), targets.cpu()\n",
    "iteration = 0\n",
    "\n",
    "def draw_inputs(inputs):\n",
    "    for index in range(len(inputs)):\n",
    "        input = inputs[index]\n",
    "        print(input.shape)\n",
    "        arr = input.reshape([input.shape[1], input.shape[2]])\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax.imshow(arr, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        if index > 5:\n",
    "            break\n",
    "\n",
    "if transform_using_crop:\n",
    "    for crop_index in range(len(inputs)):\n",
    "        draw_inputs(inputs[crop_index])\n",
    "        break\n",
    "else:\n",
    "    draw_inputs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRT9Be8cFPcB"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def old_train(epoch):\n",
    "    # 根据训练的epoch次数来降低learning rate\n",
    "    if epoch >= opt.lrd_se > 0:\n",
    "        frac = ((epoch - opt.lrd_se) // opt.lrd_s) + 1\n",
    "        decay_factor = opt.lrd_r ** frac\n",
    "        current_lr = opt.lr * decay_factor  # current_lr = opt.lr * 降低率 ^ ((epoch - 开始decay的epoch) // 每次decay的epoch num)\n",
    "        utils.set_lr(optimizer, current_lr)  # set the learning rate\n",
    "    else:\n",
    "        current_lr = opt.lr\n",
    "    if epoch < opt.lre_je:\n",
    "        current_lr *= 1.5  # 解决一开始收敛慢的问题\n",
    "    print('learning_rate: %s' % str(current_lr))\n",
    "    global Train_acc\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    cur_train_acc = 0.\n",
    "    time_start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if transform_using_crop:\n",
    "            bs, ncrops, c, h, w = np.shape(inputs)\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            targets = torch.Tensor([[target]*ncrops for target in targets]).view(-1)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE, torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        # print(\"outputs:\", outputs)\n",
    "        # print(\"targets:\", targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, 2*current_lr)  # 解决梯度爆炸 https://blog.csdn.net/u010814042/article/details/76154391\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += float(loss.data)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # torch.max() 加上dim参数后，返回值为 max_value, max_value_index\n",
    "        if target_type == 'ls':\n",
    "            ground_value = targets.data\n",
    "        elif target_type == 'fa':\n",
    "            _, ground_value = torch.max(targets.data, 1)\n",
    "        # print(\"predicted:\", predicted)\n",
    "        # print(\"ground_value:\", ground_value)\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i] == ground_value[i]:\n",
    "                train_acc_map[predicted[i].item()] += 1\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(ground_value.data).cpu().sum()\n",
    "        # print(\"equal: \", predicted.eq(ground_value.data).cpu())\n",
    "        cur_train_acc = float(correct) / float(total) * 100.\n",
    "\n",
    "        time_end = time.time()\n",
    "        duration = time_end - time_start\n",
    "        utils.progress_bar(batch_idx, len(train_loader), 'Time: %.2fs | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                           (duration, train_loss / (batch_idx + 1), cur_train_acc, correct, total))\n",
    "\n",
    "        # 删除无用的变量，释放显存\n",
    "        del loss\n",
    "        del inputs\n",
    "        del outputs\n",
    "        del predicted\n",
    "    Train_acc = cur_train_acc\n",
    "    write_history('Train', epoch, cur_train_acc, train_loss / (batch_idx + 1), None)\n",
    "\n",
    "\n",
    "# Testing\n",
    "def old_test(epoch):\n",
    "    global Test_acc\n",
    "    private_test_loss = 0\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    cur_test_acc = 0.\n",
    "    correct_map = [0, 0, 0, 0, 0, 0, 0]\n",
    "    time_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            if transform_using_crop:\n",
    "                bs, ncrops, c, h, w = np.shape(inputs)\n",
    "                targets = torch.Tensor([[target]*ncrops for target in targets]).view(-1)\n",
    "            else:\n",
    "                bs, c, h, w = np.shape(inputs)\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE, torch.long)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            private_test_loss += float(loss.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if target_type == 'ls':\n",
    "                ground_value = targets.data\n",
    "            elif target_type == 'fa':\n",
    "                _, ground_value = torch.max(targets.data, 1)\n",
    "\n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == ground_value[i]:\n",
    "                    c = predicted[i].item()\n",
    "                    test_acc_map[c] += 1\n",
    "                    correct_map[c] += 1\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(ground_value.data).cpu().sum()\n",
    "            cur_test_acc = float(correct) / float(total) * 100.\n",
    "\n",
    "            time_end = time.time()\n",
    "            duration = time_end - time_start\n",
    "            utils.progress_bar(batch_idx, len(test_loader), 'Time: %.2fs | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                               (duration, private_test_loss / (batch_idx + 1), cur_test_acc, correct, total))\n",
    "\n",
    "            # 删除无用的变量，释放显存\n",
    "            del loss\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del predicted\n",
    "\n",
    "    Test_acc = cur_test_acc\n",
    "    if test_acc_map['best_acc'] < Test_acc or (test_acc_map['best_acc'] <= Test_acc and train_acc_map['best_acc'] <= Train_acc):\n",
    "        train_acc_map['best_acc'] = Train_acc\n",
    "        train_acc_map['best_acc_epoch'] = epoch\n",
    "        test_acc_map['best_acc'] = Test_acc\n",
    "        test_acc_map['best_acc_epoch'] = epoch\n",
    "        print('Saving net to %s' % net_to_save_path)\n",
    "        print('best_acc: %0.3f' % test_acc_map['best_acc'])\n",
    "        print('correct_map: %s' % correct_map)\n",
    "        state = {'net': net.state_dict() if use_cuda else net,\n",
    "                 'best_test_acc': test_acc_map['best_acc'],\n",
    "                 'best_test_acc_epoch': test_acc_map['best_acc_epoch'],\n",
    "                 'best_train_acc': train_acc_map['best_acc'],\n",
    "                 'best_train_acc_epoch': train_acc_map['best_acc_epoch'],\n",
    "                 'cur_epoch': epoch,\n",
    "                 'correct_map': correct_map,\n",
    "                 }\n",
    "        torch.save(state, os.path.join(net_to_save_path, saved_model_name))\n",
    "    write_history('Test', epoch, cur_test_acc, private_test_loss / (batch_idx + 1), correct_map)\n",
    "        \n",
    "        \n",
    "# Training\n",
    "def train(epoch):\n",
    "    print(\"---Train---\")\n",
    "    # 根据训练的epoch次数来降低learning rate\n",
    "    if epoch >= opt.lrd_se > 0:\n",
    "        frac = ((epoch - opt.lrd_se) // opt.lrd_s) + 1\n",
    "        decay_factor = opt.lrd_r ** frac\n",
    "        current_lr = opt.lr * decay_factor  # current_lr = opt.lr * 降低率 ^ ((epoch - 开始decay的epoch) // 每次decay的epoch num)\n",
    "        utils.set_lr(optimizer, current_lr)  # set the learning rate\n",
    "    else:\n",
    "        current_lr = opt.lr\n",
    "    if epoch < opt.lre_je:\n",
    "        current_lr *= 1.5  # 解决一开始收敛慢的问题\n",
    "    print('learning_rate: %s' % str(current_lr))\n",
    "    global Train_acc\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred_err_dataset = RawDataSet()\n",
    "    pred_err_map = [0, 0, 0, 0, 0, 0, 0]\n",
    "    cur_train_acc = 0.\n",
    "    time_start = time.time()\n",
    "    batch_idx = 0\n",
    "    inputs, targets = train_prefetcher.next()\n",
    "    while inputs is not None:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = put_through_net(inputs, targets)\n",
    "        # print(\"outputs:\", outputs)\n",
    "        # print(\"targets:\", targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, 2*current_lr)  # 解决梯度爆炸 https://blog.csdn.net/u010814042/article/details/76154391\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += float(loss.data)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # torch.max() 加上dim参数后，返回值为 max_value, max_value_index\n",
    "        if target_type == 'ls':\n",
    "            ground_value = targets.data\n",
    "        elif target_type == 'fa':\n",
    "            _, ground_value = torch.max(targets.data, 1)\n",
    "        # print(\"predicted:\", predicted)\n",
    "        # print(\"ground_value:\", ground_value)\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i] == ground_value[i]:\n",
    "                train_acc_map[predicted[i].item()] += 1\n",
    "            else:\n",
    "                pred_err_dataset.add(inputs[i], targets[i])\n",
    "                pred_err_map[ground_value[i].item()] += 1\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(ground_value.data).cpu().sum()\n",
    "        # print(\"equal: \", predicted.eq(ground_value.data).cpu())\n",
    "        cur_train_acc = float(correct) / float(total) * 100.\n",
    "\n",
    "        time_end = time.time()\n",
    "        duration = time_end - time_start\n",
    "        utils.progress_bar(batch_idx, len(train_loader), 'Time: %.2fs | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                           (duration, train_loss / (batch_idx + 1), cur_train_acc, correct, total))\n",
    "        \n",
    "        # 删除无用的变量，释放显存\n",
    "        del loss\n",
    "        del inputs\n",
    "        del outputs\n",
    "        del predicted\n",
    "        \n",
    "        inputs, targets = train_prefetcher.next()\n",
    "        batch_idx += 1\n",
    "        \n",
    "    Train_acc = cur_train_acc\n",
    "    write_history('Train', epoch, cur_train_acc, train_loss / (batch_idx + 1), None)\n",
    "    \n",
    "    pred_err_loop(current_lr/opt.pred_err_lr_decay, pred_err_dataset, pred_err_map)\n",
    "    del pred_err_dataset\n",
    "    \n",
    "        \n",
    "def pred_err_loop(current_lr, pred_err_dataset, pred_err_map):\n",
    "    '''\n",
    "    loop循环，将预测错的继续训练\n",
    "    :param current_lr: 学习率\n",
    "    :param pred_err_dataset: 预测错误的数据集\n",
    "    :param pred_err_map: 预测错误的类别数量\n",
    "    :return: None\n",
    "    '''\n",
    "    pred_err_epoch = 1\n",
    "    pred_err_epoch_max = opt.pred_err_epoch_max\n",
    "    pred_err_dataset_temp = None\n",
    "    while len(pred_err_dataset) > 0 and pred_err_epoch <= pred_err_epoch_max:\n",
    "        print(\"pred_err_loop:%d, err_num:%d, err_map:%s, current_lr：%s\" % (pred_err_epoch, len(pred_err_dataset), pred_err_map, str(current_lr))))\n",
    "        pred_err_epoch += 1\n",
    "        pred_err_map = [0, 0, 0, 0, 0, 0, 0]\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        time_start = time.time()\n",
    "        batch_idx = 0\n",
    "        del pred_err_dataset_temp\n",
    "        pred_err_dataset_temp = pred_err_dataset\n",
    "        pred_err_dataset = RawDataSet()\n",
    "        pred_err_loader = torch.utils.data.DataLoader(pred_err_dataset_temp, batch_size=opt.bs, shuffle=True)\n",
    "        pred_err_prefetcher = Prefetcher(pred_err_loader)\n",
    "        inputs, targets = pred_err_prefetcher.next()\n",
    "        while inputs is not None:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = put_through_net(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            utils.clip_gradient(optimizer, 2*current_lr)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += float(loss.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if target_type == 'ls':\n",
    "                ground_value = targets.data\n",
    "            elif target_type == 'fa':\n",
    "                _, ground_value = torch.max(targets.data, 1)\n",
    "\n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] != ground_value[i]:\n",
    "                    pred_err_dataset.add(inputs[i], targets[i])\n",
    "                    pred_err_map[ground_value[i].item()] += 1\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(ground_value.data).cpu().sum()\n",
    "            cur_train_acc = float(correct) / float(total) * 100.\n",
    "\n",
    "            time_end = time.time()\n",
    "            duration = time_end - time_start\n",
    "            utils.progress_bar(batch_idx, len(pred_err_loader), 'Time: %.2fs | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                               (duration, train_loss / (batch_idx + 1), cur_train_acc, correct, total))\n",
    "\n",
    "            # 删除无用的变量，释放显存\n",
    "            del loss\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del predicted\n",
    "\n",
    "            inputs, targets = pred_err_prefetcher.next()\n",
    "            batch_idx += 1\n",
    "    del pred_err_dataset\n",
    "    del pred_err_dataset_temp\n",
    "        \n",
    "        \n",
    "# Testing\n",
    "def test(epoch):\n",
    "    print(\"---Test---\")\n",
    "    global Test_acc\n",
    "    private_test_loss = 0\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    cur_test_acc = 0.\n",
    "    correct_map = [0, 0, 0, 0, 0, 0, 0]\n",
    "    time_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        batch_idx = 0\n",
    "        inputs, targets = test_prefetcher.next()\n",
    "        while inputs is not None:\n",
    "            outputs = put_through_net(inputs, targets)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            private_test_loss += float(loss.data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if target_type == 'ls':\n",
    "                ground_value = targets.data\n",
    "            elif target_type == 'fa':\n",
    "                _, ground_value = torch.max(targets.data, 1)\n",
    "\n",
    "            for i in range(len(predicted)):\n",
    "                if predicted[i] == ground_value[i]:\n",
    "                    c = predicted[i].item()\n",
    "                    test_acc_map[c] += 1\n",
    "                    correct_map[c] += 1\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(ground_value.data).cpu().sum()\n",
    "            cur_test_acc = float(correct) / float(total) * 100.\n",
    "\n",
    "            time_end = time.time()\n",
    "            duration = time_end - time_start\n",
    "            utils.progress_bar(batch_idx, len(test_loader), 'Time: %.2fs | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                               (duration, private_test_loss / (batch_idx + 1), cur_test_acc, correct, total))\n",
    "\n",
    "            # 删除无用的变量，释放显存\n",
    "            del loss\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del predicted\n",
    "            \n",
    "            inputs, targets = test_prefetcher.next()\n",
    "            batch_idx += 1\n",
    "\n",
    "    Test_acc = cur_test_acc\n",
    "    if test_acc_map['best_acc'] < Test_acc or (test_acc_map['best_acc'] <= Test_acc and train_acc_map['best_acc'] <= Train_acc):\n",
    "        train_acc_map['best_acc'] = Train_acc\n",
    "        train_acc_map['best_acc_epoch'] = epoch\n",
    "        test_acc_map['best_acc'] = Test_acc\n",
    "        test_acc_map['best_acc_epoch'] = epoch\n",
    "        print('Saving net to %s' % net_to_save_path)\n",
    "        print('best_acc: %0.3f' % test_acc_map['best_acc'])\n",
    "        print('correct_map: %s' % correct_map)\n",
    "        state = {'net': net.state_dict() if use_cuda else net,\n",
    "                 'best_test_acc': test_acc_map['best_acc'],\n",
    "                 'best_test_acc_epoch': test_acc_map['best_acc_epoch'],\n",
    "                 'best_train_acc': train_acc_map['best_acc'],\n",
    "                 'best_train_acc_epoch': train_acc_map['best_acc_epoch'],\n",
    "                 'cur_epoch': epoch,\n",
    "                 'correct_map': correct_map,\n",
    "                 }\n",
    "        torch.save(state, os.path.join(net_to_save_path, saved_model_name))\n",
    "    write_history('Test', epoch, cur_test_acc, private_test_loss / (batch_idx + 1), correct_map)\n",
    "\n",
    "\n",
    "def put_through_net(inputs, targets):\n",
    "    '''\n",
    "    将inputs输入net，得到输出，并返回\n",
    "    :param inputs: 网络的输入\n",
    "    :param targets: 网络输入对应的label\n",
    "    :return: 网络的输出\n",
    "    '''\n",
    "    if transform_using_crop:\n",
    "        bs, ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        targets = torch.Tensor([[target]*ncrops for target in targets]).view(-1)\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE, torch.long)\n",
    "    inputs, targets = Variable(inputs), Variable(targets)\n",
    "    outputs = net(inputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def write_history(train_or_test, epoch, acc, loss, predictions):\n",
    "    '''\n",
    "    将数据写入history.txt文件保存\n",
    "    :param train_or_test: 训练过程还是测试过程（'Train' or 'Test'）\n",
    "    :param epoch: 迭代次数\n",
    "    :param acc: 准确率\n",
    "    :param loss: 损失\n",
    "    :param predictions: 预测情况\n",
    "    :return: 无\n",
    "    '''\n",
    "    with open(os.path.join(net_to_save_path, history_file_name), \"a+\", encoding=\"utf-8\") as history_file:\n",
    "        msg = train_or_test + \" %d %.3f %.3f \" % (epoch, acc, loss)\n",
    "        if predictions:\n",
    "            msg += str(predictions)\n",
    "        msg += \"\\n\"\n",
    "        history_file.write(msg)\n",
    "        history_file.flush()\n",
    "\n",
    "\n",
    "def save_over_flag():\n",
    "    '''\n",
    "    创建一个空文件表示训练完成\n",
    "    :return: 无\n",
    "    '''\n",
    "    file_path = os.path.join(net_to_save_path, model_over_flag_name)\n",
    "    with open(file_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.write(train_acc_map.__str__())\n",
    "        file.write(\"\\n\")\n",
    "        file.write(test_acc_map.__str__())\n",
    "        file.write(\"\\n\")\n",
    "        file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8228
    },
    "colab_type": "code",
    "id": "VET9IwJvFPcE",
    "outputId": "0d3ecd67-9578-4a19-90bc-06a4d239ac57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(net_to_save_dir):\n",
    "    os.mkdir(net_to_save_dir)\n",
    "if not os.path.isdir(os.path.join(net_to_save_dir, str(opt.save_number))):\n",
    "    os.mkdir(os.path.join(net_to_save_dir, str(opt.save_number)))\n",
    "if not os.path.isdir(net_to_save_path):\n",
    "    os.mkdir(net_to_save_path)\n",
    "train_prefetcher = None\n",
    "test_prefetcher = None\n",
    "if not over_flag:\n",
    "    for epoch in range(start_epoch, opt.epoch, 1):\n",
    "        print('\\n------------Epoch: %d-------------' % epoch)\n",
    "        if use_cuda:\n",
    "            train_prefetcher = Prefetcher(train_loader)\n",
    "            test_prefetcher = Prefetcher(test_loader)\n",
    "            train(epoch)\n",
    "            test(epoch)\n",
    "        else:\n",
    "            old_train(epoch)\n",
    "            old_test(epoch)\n",
    "        temp_internal -= 1\n",
    "        if temp_internal <= 0:\n",
    "            temp_internal = TEMP_EPOCH\n",
    "            print(\"Saving Temp Model...\")\n",
    "            state = {'net': net.state_dict() if use_cuda else net,\n",
    "                 'best_test_acc': test_acc_map['best_acc'],\n",
    "                 'best_test_acc_epoch': test_acc_map['best_acc_epoch'],\n",
    "                 'best_train_acc': train_acc_map['best_acc'],\n",
    "                 'best_train_acc_epoch': train_acc_map['best_acc_epoch'],\n",
    "                 'cur_epoch': epoch,\n",
    "                 }\n",
    "            torch.save(state, os.path.join(net_to_save_path, saved_temp_model_name))\n",
    "    print(train_acc_map)\n",
    "    print(test_acc_map)\n",
    "    save_over_flag()\n",
    "print(\"Trained Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
